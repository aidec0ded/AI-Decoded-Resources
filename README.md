# AI Decoded: Resources

## Paper Repository
### Foundations

* [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473)
* [Long Short-Term Memory](https://ieeexplore.ieee.org/abstract/document/6795963)
* [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)
* [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
* [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)
* [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://jmlr.org/papers/volume21/20-074/20-074.pdf)
* [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165)
* [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020)
* [Zero-Shot Text-to-Image Generation](https://arxiv.org/pdf/2102.12092)
* [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239)
* [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://cdn.openai.com/papers/dall-e-2.pdf)
* [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752)
* [Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding](https://arxiv.org/pdf/2205.11487)
* [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311)
* [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971)
* [GPT-4 Technical Report](https://arxiv.org/pdf/2303.08774)
* [Learning to Summarize with Human Feedback](https://arxiv.org/pdf/2009.01325)
* [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361)
* [Training Language Models to Follow Instructions with Human Feedback](https://arxiv.org/pdf/2203.02155)
* [Towards Understanding Sycophancy in Language Models](https://arxiv.org/pdf/2310.13548)
* [Direct Preference Optimization: Your Language Model Is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290)
